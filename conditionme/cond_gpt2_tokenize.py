from copy import copy
from typing import Sequence, List, Optional

import torch
from transformers import (
    PreTrainedTokenizerBase,
    BatchEncoding,
    TensorType,
    GPT2Tokenizer,
)

from settings import DEFAULT_REWARD_TOKEN_ID


def set_up_decoder_tokenizer(
    tokenizer: PreTrainedTokenizerBase,
) -> PreTrainedTokenizerBase:
    # shallow copy tokenizer to avoid unexpected side effects
    new_tokenizer: PreTrainedTokenizerBase = copy(tokenizer)
    # need to manually set the pad token to the eos token
    new_tokenizer.pad_token = new_tokenizer.eos_token
    new_tokenizer.pad_token_id = new_tokenizer.eos_token_id
    # since this is a decoder, we need to left pad to work with HF generate
    # https://github.com/huggingface/transformers/issues/3021#issuecomment-1231526631
    new_tokenizer.padding_side = "left"
    # we also need to truncate from the left
    new_tokenizer.truncation_side = "left"
    return new_tokenizer


def manual_keep_front_truncation(
    input_ids: List[List[int]],
    max_length: int,
) -> List[List[int]]:
    # Because the transformers package truncates by keeping the last tokens, rather than the first tokens
    # we need to manually truncate the text
    # TODO: Check whether this is really true?
    return [row[:max_length] for row in input_ids]


def batch_tokenize_gpt2(
    text: Sequence[str],
    target_rewards: Sequence[float],
    tokenizer: PreTrainedTokenizerBase,
    add_eos_at_end: bool,
    # NOTE: Avoid using special tokens like eos_token, bos_token, because those may get masked automatically by data_collator
    reward_token_id: int = DEFAULT_REWARD_TOKEN_ID,
) -> BatchEncoding:
    # TODO: Do the padding in the data collator instead?
    # shallow copy tokenizer to avoid unexpected side effects
    new_tokenizer: PreTrainedTokenizerBase = set_up_decoder_tokenizer(tokenizer)
    # TODO: implement truncation from the LHS instead of the RHS
    assert len(text) == len(target_rewards)
    # add the reward token to the start of all text, before we apply the padding
    # the `forward` method of ModifiedGPT2LMHeadModel will modify the embedding of the reward_token using the position provided
    reward_token = new_tokenizer.decode([reward_token_id])
    tokenized_ids = new_tokenizer(text)["input_ids"]
    # add reward_token to the start of all text, and add eos_token to the end of all text
    tokenized_ids_with_special_tokens: List[List[int]] = [
        [reward_token_id]
        + row
        + ([new_tokenizer.eos_token_id] if add_eos_at_end else [])
        for row in tokenized_ids
    ]
    # Manually pad and truncate we want to add the token id ourselves
    tokenizer_result = new_tokenizer.pad(
        {
            "input_ids": manual_keep_front_truncation(
                tokenized_ids_with_special_tokens, max_length=tokenizer.model_max_length
            )
        },
        max_length=tokenizer.model_max_length,
        return_attention_mask=True
    )
    inputs_ids = tokenizer_result["input_ids"]
    for i, input_id_row in enumerate(inputs_ids):
        if reward_token_id not in input_id_row:
            decoded_text = new_tokenizer.decode(input_id_row)
            raise ValueError(
                f"New Text: {text[i]} did not get tokenized correctly. Got tokenize to {input_id_row}\nDecoded text {decoded_text}"
            )

    # BatchEncoding will have "input_ids", "attention_mask, "target_reward", "labels"
    # add the precomputed reward to the result
    tokenizer_result["target_reward"] = target_rewards
    # convert to tensors
    new_dict = BatchEncoding(tensor_type=TensorType.PYTORCH)
    for key in tokenizer_result:
        new_dict[key] = torch.tensor(tokenizer_result[key])
    return new_dict


def test_tokenize():
    tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
    _ids = [
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        50256,
        855,
        2625,
        40,
        1703,
        44269,
        25,
        12550,
        1,
        318,
        257,
        6106,
        856,
        290,
        2181,
        43787,
        2876,
        3723,
        14540,
        13,
        632,
        1595,
        470,
        2300,
        644,
        530,
        338,
        1964,
        5009,
        389,
        780,
        428,
        2646,
        460,
        8941,
        307,
        2077,
        6411,
        319,
        597,
        1241,
        13,
        1081,
        329,
        262,
        1624,
        326,
        30424,
        4257,
        42156,
        318,
        281,
        11353,
        8823,
        12,
        1558,
        11,
        326,
        2125,
        470,
        2081,
        13,
        314,
        1053,
        1775,
        371,
        12,
        4111,
        7328,
        351,
        4257,
        42156,
        13,
        38842,
        11,
        484,
        691,
        2897,
        617,
        42738,
        5009,
        11,
        475,
        810,
        389,
        262,
        371,
        12,
        4111,
        7328,
        351,
        46529,
        24477,
        11017,
        290,
        781,
        5912,
        2248,
        544,
        30,
        2735,
        1456,
        11,
        780,
        484,
        836,
        470,
        2152,
        13,
        383,
        976,
        2925,
        329,
        883,
        40805,
        7862,
        2523,
        25,
        5513,
        6511,
        82,
        25635,
        287,
        262,
        28633,
        475,
        407,
        257,
        48852,
        271,
        287,
        6504,
        13,
        843,
        883,
        2181,
        43787,
        19907,
        6918,
        588,
        383,
        4373,
        28683,
        11,
        287,
        543,
        356,
        821,
        5716,
        284,
        262,
        2524,
        286,
        18653,
        7096,
        78,
        338,
        48836,
        4623,
        45610,
        1559,
        11,
        475,
        407,
        257,
        12854,
        286,
        11398,
        7424,
        319,
        29476,
        37918,
        570,
        88,
        13,
        7413,
        13774,
        357,
        273,
        30253,
        8,
        366,
        23352,
        12,
        20307,
        1,
        287,
        6067,
        286,
        42156,
        11,
        262,
        14946,
        909,
        83,
        1904,
        815,
        1011,
        656,
        1848,
        530,
        34804,
        1346,
        3489,
        48631,
        3580,
        1022,
        1450,
        290,
        1466,
        25,
        612,
        389,
        645,
        35853,
        319,
        3359,
        618,
        49798,
        3568,
        26349,
        11,
        290,
        262,
        976,
        2314,
        307,
        531,
        329,
        257,
        582,
        13,
        554,
        1109,
        11,
        345,
        4143,
        1839,
        470,
        766,
        4048,
        35853,
        287,
        281,
        1605,
        2646,
        287,
        1997,
        1790,
        286,
        8483,
        393,
        7952,
        1931,
        313,
        3970,
        13,
        770,
        4260,
        4274,
        12,
        20307,
        318,
        1342,
        257,
        4274,
        3210,
        621,
        281,
        33603,
        31193,
        2694,
        284,
        1282,
        284,
        2846,
        30573,
        351,
        262,
        1035,
        1460,
        286,
        1466,
        338,
        5920,
        13,
        50256,
    ]
    text = tokenizer.decode(_ids)
    print(text)
